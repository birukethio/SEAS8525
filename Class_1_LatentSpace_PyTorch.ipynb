{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/birukethio/SEAS8525/blob/main/Class_1_LatentSpace_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code defines and utilizes an Autoencoder neural network for the MNIST dataset. Here's a detailed explanation:\n",
        "\n",
        "- **Autoencoder Class**: This class creates a neural network with encoder and decoder components. The encoder compresses the input image into a lower-dimensional latent space, and the decoder reconstructs the image from this latent space. The network is structured to flatten and process 28x28 grayscale MNIST images.\n",
        "\n",
        "- **Data Loading**: Utilizes PyTorch's DataLoader to efficiently load the MNIST dataset, applying transformations to normalize the images.\n",
        "\n",
        "- **Training Loop**: Iterates over the training dataset, feeding batches of images through the model, calculating the reconstruction loss, and updating the model's weights to minimize this loss, effectively learning to compress and reconstruct the input images.\n",
        "\n",
        "- **Visualization**: After training, the script visualizes a batch of original images and their reconstructions from the autoencoder. It also prints the latent space representations, showcasing what the model has learned to encode.\n",
        "\n",
        "- **Utility Functions**: Includes `imshow` for displaying tensors as images. It unnormalizes the data and uses Matplotlib to plot them.\n",
        "\n",
        "This script encapsulates the end-to-end process of training an autoencoder on the MNIST dataset, visualizing the results, and examining the learned latent space."
      ],
      "metadata": {
        "id": "MXble1sY6GuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "QTp8sYxwBg-1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PTGwHzWZ6QHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The `Autoencoder` class inherits from `nn.Module`, a base class for all neural network modules in PyTorch. Here's a breakdown of the code and its functionality:\n",
        "\n",
        "### Initialization Method (`__init__`)\n",
        "- **Parameters**: The `__init__` method accepts a single parameter `encoding_dim`, which specifies the size of the latent space where the input data is compressed.\n",
        "- **Encoder**: The encoder part of the autoencoder is designed to compress the input data (in this case, an image) into a lower-dimensional representation called the latent space. It consists of a sequence of layers:\n",
        "  - `nn.Linear(28 * 28, 128)`: This layer flattens the input image (assumed to be 28x28 pixels, typical for MNIST dataset images) into a one-dimensional array and performs a linear transformation to reduce its dimension to 128.\n",
        "  - `nn.ReLU(True)`: A Rectified Linear Unit (ReLU) activation function is applied to introduce non-linearity, helping the model learn complex patterns.\n",
        "  - Another `nn.Linear(128, encoding_dim)`: Further reduces the dimension from 128 to the specified `encoding_dim`.\n",
        "  - Another `nn.ReLU(True)`: Another ReLU activation for non-linearity.\n",
        "- **Decoder**: The decoder part reconstructs the original input data from the compressed representation. It mirrors the encoder structure but in reverse, aiming to expand the compressed data back to its original shape:\n",
        "  - `nn.Linear(encoding_dim, 128)`: Expands the compressed data from `encoding_dim` back to 128.\n",
        "  - `nn.ReLU(True)`: Applies ReLU activation.\n",
        "  - `nn.Linear(128, 28 * 28)`: Transforms the data from 128 back to the flattened image size of 784 (28x28).\n",
        "  - `nn.Sigmoid()`: Applies a sigmoid activation function to ensure the output values are between 0 and 1, suitable for image data where pixel values typically fall within this range.\n",
        "\n",
        "### Forward Method (`forward`)\n",
        "- **Parameter**: The `forward` method defines how the input `x` flows through the network.\n",
        "- **Process**:\n",
        "  - `x.view(-1, 28*28)`: First, the input `x` is reshaped into a one-dimensional array (flattened) if not already done.\n",
        "  - `self.encoder(x)`: The flattened `x` is then passed through the encoder.\n",
        "  - `self.decoder(x)`: The output from the encoder, which is the compressed representation, is fed into the decoder.\n",
        "- **Output**: The final output is reshaped back to the original image dimensions (`-1, 1, 28, 28`), where `-1` is a placeholder that automatically adjusts based on the batch size.\n"
      ],
      "metadata": {
        "id": "Lth1Gula4Sz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EvzvTgl46WFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: compresses the image into a lower-dimensional latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),  # Flatten the image and then linearly transform it\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, encoding_dim),  # Linear transformation to the encoding dimension\n",
        "            nn.ReLU(True)  # Non-linear activation function\n",
        "        )\n",
        "        # Decoder: reconstructs the image from the latent space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),  # Linearly transforms the encoding\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, 28 * 28),  # Transforms back to original image shape\n",
        "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x.view(-1, 28*28))  # Encode the input image\n",
        "        x = self.decoder(x)  # Decode the encoded image\n",
        "        return x.view(-1, 1, 28, 28)  # Reshape to the original image dimensions"
      ],
      "metadata": {
        "id": "wWlX07fEBjL8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def imshow(img):\n",
        "    img = img.cpu() / 2 + 0.5  # Unnormalize the image\n",
        "    npimg = img.numpy()  # Convert the tensor to a numpy array\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Reshape and display the image\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yP7S8PazBoD5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform: converts images to PyTorch tensors and normalizes them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "yJAgqh0TBrMa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset loading\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "cQpswSgnBw4E",
        "outputId": "811098ae-ec31-496e-c9e2-aeee8694a45d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.14MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.15MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(encoding_dim=64).to(device)\n",
        "criterion = nn.MSELoss()  # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Optimizer"
      ],
      "metadata": {
        "id": "W8NemtCtB35p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        img, _ = data\n",
        "        img = img.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            output = model(img)\n",
        "            loss = criterion(output, img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRk72snY6LPD",
        "outputId": "f16d7944-ef2a-4c62-a264-cf87b2035b47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1361577664.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-1361577664.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss:0.9247\n",
            "Epoch [2/10], Loss:0.9246\n",
            "Epoch [3/10], Loss:0.9249\n",
            "Epoch [4/10], Loss:0.9246\n",
            "Epoch [5/10], Loss:0.9246\n",
            "Epoch [6/10], Loss:0.9301\n",
            "Epoch [7/10], Loss:0.9248\n",
            "Epoch [8/10], Loss:0.9233\n",
            "Epoch [9/10], Loss:0.9213\n",
            "Epoch [10/10], Loss:0.9255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the original and reconstructed images\n",
        "dataiter = iter(train_loader)\n",
        "images, _ = next(dataiter)\n",
        "images = images.to(device)\n",
        "\n",
        "# Display original images\n",
        "print(\"Original Images\")\n",
        "imshow(vutils.make_grid(images[:4], normalize=True))\n",
        "\n",
        "# Encoded and decoded images\n",
        "with torch.no_grad():\n",
        "    encoded_imgs = model.encoder(images.view(-1, 28*28)[:4])\n",
        "    decoded_imgs = model.decoder(encoded_imgs).view(-1, 1, 28, 28)\n",
        "\n",
        "\n",
        "# Display reconstructed images\n",
        "print(\"Original Representation\")\n",
        "print(img)\n",
        "\n",
        "# Print latent space\n",
        "print(\"Latent space representations:\")\n",
        "print(encoded_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FCsJeFk2B2Fi",
        "outputId": "18001de5-a613-4a18-9231-38b58fef0ab8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHM1JREFUeJzt3XtwlNX9x/FPAslyyc2ASQghkgpykXALECNULaRSvICFWrWo8TK12mCFzFSlLTra2qidFqsittbBdhSxdAQKU1EMGqRyCQlYMBhQMxIJCVXMhQBJTJ7fH5b9eU5ikk022SfJ+zWzM352n3325OTC12e/e06Q4ziOAAAAXCA40AMAAAA4i8IEAAC4BoUJAABwDQoTAADgGhQmAADANShMAACAa1CYAAAA16AwAQAArkFhAgAAXIPCBAAAuEanFSYrVqzQ8OHD1a9fP6Wmpmr37t2d9VIAAKCHCOqMvXJeeeUV3XzzzXr22WeVmpqqJ554QmvXrlVRUZFiYmJafG5jY6NKS0sVHh6uoKAgfw8NAAB0AsdxVF1drfj4eAUHt/+6R6cUJqmpqZo6daqefvppSV8VG8OGDdPdd9+t+++/v8Xnfvrppxo2bJi/hwQAALpASUmJEhIS2v38vn4ciySprq5O+fn5Wrp0qfe+4OBgpaena8eOHU2Or62tVW1trTefrZOWLFkij8fj7+EBAIBOUFtbq+XLlys8PLxD5/F7YfLZZ5+poaFBsbGxxv2xsbH64IMPmhyfnZ2thx56qMn9Ho+HwgQAgG6mo20YAf9UztKlS1VZWem9lZSUBHpIAAAgQPx+xWTw4MHq06ePysvLjfvLy8sVFxfX5HiujAAAgLP8fsUkNDRUKSkpysnJ8d7X2NionJwcpaWl+fvlAABAD+L3KyaSlJWVpYyMDE2ZMkXTpk3TE088oZqaGt16662d8XIAAKCH6JTC5LrrrtN///tfPfDAAyorK9PEiRO1efPmJg2xAAAAX9cphYkkLVq0SIsWLeqs0wMAgB4o4J/KAQAAOIvCBAAAuAaFCQAAcA0KEwAA4BoUJgAAwDUoTAAAgGtQmAAAANegMAEAAK5BYQIAAFyDwgQAALgGhQkAAHANChMAAOAaFCYAAMA1KEwAAIBrUJgAAADX6BvoAQAATMHB5v8zhoWFGXnQoEFGvvTSS408fPjwJud0HMfIL774opE/+ugjX4cJdAqumAAAANegMAEAAK5BYQIAAFyDHhOgB5k4caKRw8PDjTx+/Pgmzzn33HNbPGdOTo6R33nnnfYNrhcLDQ01ckxMjJHj4+ONPGLEiBZza+x+kubus89JjwncgismAADANShMAACAa1CYAAAA16DHJED69jWn/tvf/raRJ0yYYOSoqCgjl5aWGjk6OtrI9hoFn376aXuGiS5m9xokJCQYeciQIUYeO3askUNCQowcFBTU6mvW1tYaed++fUa2f9bQlL2uSGpqqpHPO+88I7fW12N/35rrGfGV/TcgLy+vw+cEOgNXTAAAgGtQmAAAANegMAEAAK5Bj0kXOf/884181VVXGdnuIbGdPHnSyP379zey/R70woULjbx8+XIj19XVtfh68A+758PuCbF/LkaNGmVke/0LW2VlpZEPHjxo5KqqKiM311dg/+zYP2to6vrrrzey3UPSr18/I/ujR+TrDhw4YOTDhw8b+ZNPPmnynFOnThn5yy+/9OuYuiN7T6LBgwcb2V5vZvTo0Ua+8MILO/T677//vpH/8Y9/dOh8PQVXTAAAgGtQmAAAANfwuTDZtm2brr76asXHxysoKEjr1683HnccRw888ICGDBmi/v37Kz09vcllRgAAgOb43GNSU1OjCRMm6LbbbtP8+fObPP7444/rySef1F//+lclJSVp2bJlmj17tgoLC5u879qT2euQXHHFFUa2ewfsXoDNmzcbubi42Mhnzpwx8rhx44y8YMECI9vrpNj7n6B9EhMTjWyvT3HxxRcb2V5vxvb5558bee/evUYuKioyst0rZP8cwT/sdYciIiKM3NrfNntdkpqaGiPba4zYPSKHDh0ysv1zgqaa+57YPV0pKSlGTkpKavGc9po/do+Izd5/KDk52cgXXHCBkefOnWvkf/3rX0buLX1BPhcmc+bM0Zw5c5p9zHEcPfHEE/rVr36lefPmSZL+9re/KTY2VuvXr2/SMAYAAPB1fu0xKS4uVllZmdLT0733RUZGKjU1VTt27Gj2ObW1taqqqjJuAACgd/JrYVJWViZJio2NNe6PjY31PmbLzs5WZGSk9zZs2DB/DgkAAHQjAV/HZOnSpcrKyvLmqqqqblmcDBgwwMhXXnmlke31LOxC7aWXXjKyr2tJfPzxx0Y+ffq0kWfMmNHi45L07rvv+vSavdHQoUONbL+tGRcX59P57HVFtm/fbmSuIAaGva7Q9OnTjWx/n+11ShobG42cm5tr5Pz8fCPba4ygdXYPib2P1GWXXdbkOXZPmP37ZX+f9u/fb2S7N8ju9WuNfb4bb7zRyJMmTTKy3Uv073//26fX6678esXk7C9reXm5cX95efk3/sH2eDyKiIgwbgAAoHfya2GSlJSkuLg44xMfVVVV2rVrl9LS0vz5UgAAoAfy+a2ckydP6sMPP/Tm4uJi7du3T9HR0UpMTNTixYv1m9/8RiNHjvR+XDg+Pl7XXHONP8cNAAB6IJ8Lkz179ug73/mON5/tD8nIyNALL7yge++9VzU1NbrjjjtUUVGhGTNmaPPmzT1+DRN7zwS7p8R+e6ujPSW2yZMnG7m1+bbXTUDz7Pekf/SjHxnZ4/H4dL76+noj23099JQExqBBg4x8yy23GHngwIEtPt/uBbA/hVhQUND+wUFS031sbrrpJiPbbQDN7Tuza9cuI9vrw3T2OiH2+e11Su666y4j2/+O9BY+FyaXXXZZixtSBQUF6eGHH9bDDz/coYEBAIDeh71yAACAa1CYAAAA1wj4OiY9xaxZs4xs741h73XT0Z4Se++bmTNntni8/Z73kSNHOvT6vcUXX3xhZHvTSnutBHtxQdu+ffuMXFFR0c6RwZ8uvfRSI4eFhRnZfvva7il58803jWzvaQT/s3tKjh8/buTW9rGBe3HFBAAAuAaFCQAAcA0KEwAA4Br0mPjJ1xedk5quazJy5Egjv/fee0a291yw3+O2V84dO3asT+Pbtm2bT8fjK9XV1Ua238f2dR0T+3wIjCuuuMLIycnJLR5v94zZ2f45CA8PNzLf94777LPPjPzQQw8FaCRdZ/fu3YEeQkBwxQQAALgGhQkAAHANChMAAOAa9Jj4SX5+vpFHjBhhZHsvjp/85CedOp7S0lIj19XVderr9RYnTpwwst1zMmDAACOHhoYa2V4vw+5FsNl7Gu3fv9/ILW0PgW82ZMgQI9vzaPeQ2I9HR0cb2d6k1N4T5fDhw0Y+cOCAkY8dO2Zk1rfpHezeQXyFKyYAAMA1KEwAAIBrUJgAAADXoMfET+y9cDZs2GBke08V26lTp4xsvydtvwf9gx/8wMh2b0NBQYGRGxsbW3x9tM/LL79sZPv7MHnyZCOPGjXKyFOnTm3x/Pbjffr0MfLevXvbNE50rZCQECOPGTOmxWz/vts/V+gZhg4dauSJEyca2e49On36dGcPyZW4YgIAAFyDwgQAALgGhQkAAHANChMAAOAaNL92koMHDxq5qKioxeNba069+OKLjWw3Wdo++uijFh9H57CbmLdv395ithfimzFjhpHPO+88I8+dO9fIJSUlRrY3OkPz1qxZY2S7ydhuHrc31bQXTLQXbLPZC+m1tsnnj3/8YyM/99xzLZ4f3YPd/Gov5Gcv1NlbF1DkigkAAHANChMAAOAaFCYAAMA16DHpIh1d4Cw5ObnFx+2eFjYB6x4+/PBDI9sL9d17771GtjcFRPvU1NQY+e23327x+KqqKiPbm2Tamyu2xu4FuuSSS4wcHx9v5EmTJhmZhfW6p3PPPdfI9gJq9s9Zb8UVEwAA4BoUJgAAwDUoTAAAgGvQY+JS9uZOcXFxRrZ7VrZu3drZQ0IXaGhoMLK97snMmTONbK97sn79+k4ZF/wrNzfXyPY6KVOmTDEyPSbdU0JCgpHHjRtn5JMnTxr5xIkTnT6m7oArJgAAwDV8Kkyys7M1depUhYeHKyYmRtdcc02TFU3PnDmjzMxMDRo0SGFhYVqwYIHKy8v9OmgAANAz+VSY5ObmKjMzUzt37tSWLVtUX1+vyy+/3Pjo3ZIlS7Rx40atXbtWubm5Ki0t1fz58/0+cAAA0PP41GOyefNmI7/wwguKiYlRfn6+LrnkElVWVur555/X6tWrve+Fr1q1SmPGjNHOnTt10UUX+W/kPVxKSkqLjxcWFhqZPVJ6ptra2hYf93g8XTQSdKbWegtiYmJazMePH/f7mNBxaWlpRu7Xr5+R33jjja4cTrfRoR6TyspKSVJ0dLSkrzYgqq+vV3p6uveY0aNHKzExUTt27OjISwEAgF6g3Z/KaWxs1OLFizV9+nRvp3FZWZlCQ0MVFRVlHBsbG6uysrJmz1NbW2v8XyEr3wEA0Hu1+4pJZmamDhw40GT7cF9lZ2crMjLSexs2bFiHzgcAALqvdl0xWbRokTZt2qRt27YZn9OOi4tTXV2dKioqjKsm5eXlTdbhOGvp0qXKysry5qqqql5ZnNg9JUOHDm3x+Pz8/M4cDtBt2fsJXXDBBU2OOXDgQFcNxy/sr4k9k9wpIiLCyKNGjTLy0aNHjcx6NM3z6YqJ4zhatGiR1q1bp61btyopKcl4PCUlRSEhIcrJyfHeV1RUpCNHjjRpAjrL4/EoIiLCuAEAgN7JpysmmZmZWr16tTZs2KDw8HBv30hkZKT69++vyMhI3X777crKylJ0dLQiIiJ09913Ky0tjU/kAACAVvlUmKxcuVKSdNlllxn3r1q1Srfccoskafny5QoODtaCBQtUW1ur2bNn65lnnvHLYAEAQM/mU2HiOE6rx/Tr108rVqzQihUr2j2o3ig5OdnIQUFBRrbXKTly5EinjwnuV1BQEOghuI69VsSCBQuaHFNfX2/kY8eOGfnMmTNGrqur82kMwcHmu+T2Xjj27/sll1zS4vnsvbHsPZXgDtdee62R+/TpY2R7pXQ0j71yAACAa1CYAAAA16AwAQAArtHulV/RMcOHDzdyfHx8i8fv3r3byPZ7zmjdwIEDjfz1zSfdYsCAAUaeMGFCi8d/8cUXnTmcHqG53rjrrruuxefYe898/vnnPr1mSEiIkUeMGOHT822HDh0yst0Tg8A4ux3LWfYeRvY6JWzN0jZcMQEAAK5BYQIAAFyDwgQAALgGPSYBYveY2O9J206cONGJo+kd7L6C5voz3nnnHSPb8+5rb09r61mcf/75Rp42bZqR7T2m7PVrTp486dN40DaxsbFGtnsHWmOvQ9SWNaC+zv4+b9q0yafno3PYv79nFxY9y/47XlhYaOQvv/yyU8bV03DFBAAAuAaFCQAAcA0KEwAA4Br0mHQRe8+E1tY1KC4uNvJHH33k9zH1Nh988IGRm9ufxN7D5MCBA0a291Bpjf2e88SJE1s83u5N+PDDD4387rvvdmg8vYHdd/OXv/ylyTHjxo0z8uTJk43s8Xj8P7CvsX+utm3bZuSqqioj+7pXDzrHrFmzjBweHm7kN99808j27y/ahismAADANShMAACAa1CYAAAA16DHpIvY70UOHTq0xeMPHjzYmcPplez+DDtL0rx584zcWk+Iv+Xk5BjZHmNDQ0NXDqdbsteaKS0tbXKMfd8bb7zRqWNC93TVVVcZ2d67Kj8/38h5eXmdPqbegCsmAADANShMAACAa1CYAAAA16AwAQAArkHzaxc5deqUkY8ePWpkuxmWTfsCY8OGDS1mAD2XvammvRCfvfAlmyt2Dq6YAAAA16AwAQAArkFhAgAAXIMeky5ib8LV3MZiAIDASUhIMHJlZaWR7c0W0Tm4YgIAAFyDwgQAALgGhQkAAHANekwAAJCUm5vbYkbX4IoJAABwDZ8Kk5UrV2r8+PGKiIhQRESE0tLS9Nprr3kfP3PmjDIzMzVo0CCFhYVpwYIFKi8v9/ugAQBAz+RTYZKQkKBHH31U+fn52rNnj2bOnKl58+bp/ffflyQtWbJEGzdu1Nq1a5Wbm6vS0lLNnz+/UwYOAAB6Hp96TK6++mojP/LII1q5cqV27typhIQEPf/881q9erVmzpwpSVq1apXGjBmjnTt36qKLLvLfqAEAQI/U7h6ThoYGrVmzRjU1NUpLS1N+fr7q6+uVnp7uPWb06NFKTEzUjh07vvE8tbW1qqqqMm4AAKB38rkw2b9/v8LCwuTxeHTnnXdq3bp1Gjt2rMrKyhQaGqqoqCjj+NjYWJWVlX3j+bKzsxUZGem9DRs2zOcvAgAA9Aw+FyajRo3Svn37tGvXLt11113KyMhQYWFhuwewdOlSVVZWem8lJSXtPhcAAOjefF7HJDQ0VCNGjJAkpaSkKC8vT3/84x913XXXqa6uThUVFcZVk/LycsXFxX3j+Twejzwej+8jBwAAPU6H1zFpbGxUbW2tUlJSFBISopycHO9jRUVFOnLkiNLS0jr6MgAAoBfw6YrJ0qVLNWfOHCUmJqq6ulqrV6/W22+/rddff12RkZG6/fbblZWVpejoaEVEROjuu+9WWloan8gBAABt4lNhcvz4cd188806duyYIiMjNX78eL3++uv67ne/K0lavny5goODtWDBAtXW1mr27Nl65plnfBqQ4ziSvvq0DgAA6B7O/rt99t/x9gpyOnoGP/v000/5ZA4AAN1USUmJEhIS2v181xUmjY2NKi0tleM4SkxMVElJiSIiIgI9rG6rqqpKw4YNYx47gDnsOObQP5jHjmMOO+6b5tBxHFVXVys+Pl7Bwe1vYXXd7sLBwcFKSEjwLrR2dl8edAzz2HHMYccxh/7BPHYcc9hxzc1hZGRkh8/L7sIAAMA1KEwAAIBruLYw8Xg8evDBB1l8rYOYx45jDjuOOfQP5rHjmMOO6+w5dF3zKwAA6L1ce8UEAAD0PhQmAADANShMAACAa1CYAAAA13BtYbJixQoNHz5c/fr1U2pqqnbv3h3oIblWdna2pk6dqvDwcMXExOiaa65RUVGRccyZM2eUmZmpQYMGKSwsTAsWLFB5eXmARux+jz76qIKCgrR48WLvfcxh2xw9elQ33nijBg0apP79+ys5OVl79uzxPu44jh544AENGTJE/fv3V3p6ug4fPhzAEbtLQ0ODli1bpqSkJPXv31/nn3++fv3rXxv7jzCHpm3btunqq69WfHy8goKCtH79euPxtszXiRMntHDhQkVERCgqKkq33367Tp482YVfReC1NI/19fW67777lJycrIEDByo+Pl4333yzSktLjXP4Yx5dWZi88sorysrK0oMPPqiCggJNmDBBs2fP1vHjxwM9NFfKzc1VZmamdu7cqS1btqi+vl6XX365ampqvMcsWbJEGzdu1Nq1a5Wbm6vS0lLNnz8/gKN2r7y8PP3pT3/S+PHjjfuZw9Z98cUXmj59ukJCQvTaa6+psLBQv//973XOOed4j3n88cf15JNP6tlnn9WuXbs0cOBAzZ49W2fOnAngyN3jscce08qVK/X000/r4MGDeuyxx/T444/rqaee8h7DHJpqamo0YcIErVixotnH2zJfCxcu1Pvvv68tW7Zo06ZN2rZtm+64446u+hJcoaV5PHXqlAoKCrRs2TIVFBTo1VdfVVFRkebOnWsc55d5dFxo2rRpTmZmpjc3NDQ48fHxTnZ2dgBH1X0cP37ckeTk5uY6juM4FRUVTkhIiLN27VrvMQcPHnQkOTt27AjUMF2purraGTlypLNlyxbn0ksvde655x7HcZjDtrrvvvucGTNmfOPjjY2NTlxcnPO73/3Oe19FRYXj8Xicl19+uSuG6HpXXnmlc9tttxn3zZ8/31m4cKHjOMxhayQ569at8+a2zFdhYaEjycnLy/Me89prrzlBQUHO0aNHu2zsbmLPY3N2797tSHI++eQTx3H8N4+uu2JSV1en/Px8paene+8LDg5Wenq6duzYEcCRdR+VlZWSpOjoaElSfn6+6uvrjTkdPXq0EhMTmVNLZmamrrzySmOuJOawrf75z39qypQpuvbaaxUTE6NJkybpueee8z5eXFyssrIyYx4jIyOVmprKPP7PxRdfrJycHB06dEiS9N5772n79u2aM2eOJObQV22Zrx07digqKkpTpkzxHpOenq7g4GDt2rWry8fcXVRWViooKEhRUVGS/DePrtvE77PPPlNDQ4NiY2ON+2NjY/XBBx8EaFTdR2NjoxYvXqzp06dr3LhxkqSysjKFhoZ6f3jOio2NVVlZWQBG6U5r1qxRQUGB8vLymjzGHLbNxx9/rJUrVyorK0u/+MUvlJeXp5/97GcKDQ1VRkaGd66a+/1mHr9y//33q6qqSqNHj1afPn3U0NCgRx55RAsXLpQk5tBHbZmvsrIyxcTEGI/37dtX0dHRzOk3OHPmjO677z7dcMMN3o38/DWPritM0DGZmZk6cOCAtm/fHuihdCslJSW65557tGXLFvXr1y/Qw+m2GhsbNWXKFP32t7+VJE2aNEkHDhzQs88+q4yMjACPrnv4+9//rpdeekmrV6/WhRdeqH379mnx4sWKj49nDuEK9fX1+uEPfyjHcbRy5Uq/n991b+UMHjxYffr0afJph/LycsXFxQVoVN3DokWLtGnTJr311ltKSEjw3h8XF6e6ujpVVFQYxzOn/y8/P1/Hjx/X5MmT1bdvX/Xt21e5ubl68skn1bdvX8XGxjKHbTBkyBCNHTvWuG/MmDE6cuSIJHnnit/vb/bzn/9c999/v66//nolJyfrpptu0pIlS5SdnS2JOfRVW+YrLi6uyYcrvvzyS504cYI5tZwtSj755BNt2bLFe7VE8t88uq4wCQ0NVUpKinJycrz3NTY2KicnR2lpaQEcmXs5jqNFixZp3bp12rp1q5KSkozHU1JSFBISYsxpUVGRjhw5wpz+z6xZs7R//37t27fPe5syZYoWLlzo/W/msHXTp09v8lH1Q4cO6bzzzpMkJSUlKS4uzpjHqqoq7dq1i3n8n1OnTik42PzT3KdPHzU2NkpiDn3VlvlKS0tTRUWF8vPzvcds3bpVjY2NSk1N7fIxu9XZouTw4cN68803NWjQIONxv81jO5p1O92aNWscj8fjvPDCC05hYaFzxx13OFFRUU5ZWVmgh+ZKd911lxMZGem8/fbbzrFjx7y3U6dOeY+58847ncTERGfr1q3Onj17nLS0NCctLS2Ao3a/r38qx3GYw7bYvXu307dvX+eRRx5xDh8+7Lz00kvOgAEDnBdffNF7zKOPPupERUU5GzZscP7zn/848+bNc5KSkpzTp08HcOTukZGR4QwdOtTZtGmTU1xc7Lz66qvO4MGDnXvvvdd7DHNoqq6udvbu3evs3bvXkeT84Q9/cPbu3ev9tEhb5ut73/ueM2nSJGfXrl3O9u3bnZEjRzo33HBDoL6kgGhpHuvq6py5c+c6CQkJzr59+4x/a2pra73n8Mc8urIwcRzHeeqpp5zExEQnNDTUmTZtmrNz585AD8m1JDV7W7VqlfeY06dPOz/96U+dc845xxkwYIDz/e9/3zl27FjgBt0N2IUJc9g2GzdudMaNG+d4PB5n9OjRzp///Gfj8cbGRmfZsmVObGys4/F4nFmzZjlFRUUBGq37VFVVOffcc4+TmJjo9OvXz/nWt77l/PKXvzT++DOHprfeeqvZv4EZGRmO47Rtvj7//HPnhhtucMLCwpyIiAjn1ltvdaqrqwPw1QROS/NYXFz8jf/WvPXWW95z+GMegxzna8sJAgAABJDrekwAAEDvRWECAABcg8IEAAC4BoUJAABwDQoTAADgGhQmAADANShMAACAa1CYAAAA16AwAQAArkFhAgAAXIPCBAAAuAaFCQAAcI3/A9mR7TA1+s2YAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Representation\n",
            "tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')\n",
            "Latent space representations:\n",
            "tensor([[23.2241, 24.3779,  2.7908,  0.0000,  0.0000, 34.7893,  0.0000,  0.0000,\n",
            "         16.5442, 23.1418, 41.2055, 31.7196, 32.0296,  0.0000, 29.0799,  6.1960,\n",
            "         14.2523,  0.0000,  0.0000,  0.0000, 20.6371,  0.0000, 27.5035,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 23.8958,  0.0000,  0.0000, 28.7016,  0.0000,\n",
            "          0.0000,  0.0000, 10.9676,  0.0000, 21.0670,  0.0000,  0.0000, 29.2775,\n",
            "         34.1222, 15.6528, 16.5454, 46.9978,  0.0000,  0.0000,  0.0000, 15.6308,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 32.5247,  0.0000,  0.0000,\n",
            "         21.6413, 12.9780, 46.0885, 16.8071, 28.8520, 16.0037, 37.4594, 15.1810],\n",
            "        [22.1519, 23.1600,  2.5186,  0.0000,  0.0000, 33.2864,  0.0000,  0.0000,\n",
            "         15.7162, 22.0992, 39.1846, 30.1287, 30.2728,  0.0000, 27.7731,  5.6310,\n",
            "         13.7124,  0.0000,  0.0000,  0.0000, 19.5149,  0.0000, 26.1964,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 22.6337,  0.0000,  0.0000, 27.3363,  0.0000,\n",
            "          0.0000,  0.0000, 10.2512,  0.0000, 19.8461,  0.0000,  0.0000, 27.7315,\n",
            "         32.6025, 14.7606, 15.7168, 44.8090,  0.0000,  0.0000,  0.0000, 14.7195,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 30.8182,  0.0000,  0.0000,\n",
            "         20.3382, 12.3089, 43.8300, 15.7710, 27.2214, 15.1900, 35.3854, 14.4526],\n",
            "        [20.3680, 21.0742,  2.5804,  0.0000,  0.0000, 30.3232,  0.0000,  0.0000,\n",
            "         14.6860, 20.3962, 35.5886, 27.4661, 27.9070,  0.0000, 25.4850,  5.4107,\n",
            "         12.8141,  0.0000,  0.0000,  0.0000, 17.8463,  0.0000, 23.6087,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 20.6757,  0.0000,  0.0000, 24.6694,  0.0000,\n",
            "          0.0000,  0.0000,  9.6825,  0.0000, 18.6810,  0.0000,  0.0000, 25.2964,\n",
            "         29.8190, 13.2860, 14.5694, 40.6400,  0.0000,  0.0000,  0.0000, 13.6457,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 28.2321,  0.0000,  0.0000,\n",
            "         18.6296, 11.0234, 40.3168, 14.6175, 25.2441, 13.5993, 32.5283, 13.0025],\n",
            "        [24.5892, 25.3943,  2.6956,  0.0000,  0.0000, 36.6615,  0.0000,  0.0000,\n",
            "         17.2499, 24.1343, 43.3321, 33.4343, 33.6232,  0.0000, 30.3922,  6.0493,\n",
            "         14.6795,  0.0000,  0.0000,  0.0000, 21.5486,  0.0000, 28.8190,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 24.7324,  0.0000,  0.0000, 30.1479,  0.0000,\n",
            "          0.0000,  0.0000, 11.1888,  0.0000, 21.6677,  0.0000,  0.0000, 30.7972,\n",
            "         35.7745, 16.4161, 17.4264, 49.4118,  0.0000,  0.0000,  0.0000, 15.9464,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 34.0724,  0.0000,  0.0000,\n",
            "         22.4536, 13.6382, 48.2861, 17.4590, 29.9377, 16.6701, 39.2483, 16.0353]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My understanding of the code.**\n",
        "\n",
        "The notebook builds a simple autoencoder for MNIST digits. The encoder flattens the 28×28 image and passes it through two Linear + ReLU layers to get a 64-dim latent vector. The decoder then tries to rebuild the image back to 28×28, ending with a Sigmoid. The data is loaded with ToTensor() and normalized to values between –1 and 1. The model is trained using MSE loss and Adam, with mixed precision (autocast, GradScaler). In training, it only prints the loss of the last batch in each epoch, not the average. There is also an imshow function to show images, and at the end the code creates latent vectors and reconstructions for a few images.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**What I observe in the results.**\n",
        "\n",
        "The loss stays around 0.92, which is quite high for MNIST. One main issue is the value range: the input images are normalized to –1 to 1, but the decoder’s Sigmoid outputs go from 0 to 1, so the network can’t match them well. Fixing that could be done by using Tanh at the output or just skipping the normalization. The latent vectors show a lot of zeros, which makes sense since ReLU cuts off negatives and makes the representation sparse. I also noticed the code makes decoded_imgs but doesn’t actually plot them. Adding a line with imshow to show reconstructions would help compare with the original images."
      ],
      "metadata": {
        "id": "kn04E5C36sE8"
      }
    }
  ]
}