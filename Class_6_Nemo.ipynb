{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNbP6ZUWOnDbq0oGVbjXJjl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/SEAS8525/blob/main/Class_6_Nemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **`import nemo.collections.asr as nemo_asr`**  \n",
        "   Pulls in NeMo’s **speech-recognition collection** and gives it the shorter nickname `nemo_asr`. NeMo is a modular NVIDIA framework; this import brings in ready-made ASR models, audio helpers, and decoding utilities.\n",
        "\n",
        "2. **`ASRModel.from_pretrained(\"nvidia/stt_en_conformer_ctc_small\")`**  \n",
        "   * Downloads a 13 M-parameter **Conformer-CTC “small” model** from NVIDIA’s model hub (Hugging Face / NGC).  \n",
        "   * Unpacks its `.nemo` checkpoint + config to `~/.cache/torch/NeMo/…` and builds a PyTorch model object on the current GPU/CPU.  \n",
        "   * Conformer is a hybrid CNN + Transformer encoder; the **CTC** head lets it emit all tokens in one pass instead of step-by-step, so it runs fast even on a Colab T4.  ([STT En Conformer-CTC Small - NGC Catalog - NVIDIA](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_ctc_small?utm_source=chatgpt.com))\n",
        "\n",
        "3. **`!wget -q https://…2086-149220-0033.wav`**  \n",
        "   Shell escape (`!`) grabs a 16 kHz WAV from LibriSpeech (the classic Harvard sentence corpus). The `-q` flag keeps wget quiet. The file ends up in `/content/` alongside your notebook.\n",
        "\n",
        "4. **Upload block (commented-out)**  \n",
        "   Shows how you could swap in your own audio via Colab’s `files.upload()`. Only one line to change: `audio_path`.\n",
        "\n",
        "5. **`audio_path = \"2086-149220-0033.wav\"`**  \n",
        "   Just sets the variable that will be fed to the model. (If you uploaded something, you’d overwrite this.)\n",
        "\n",
        "6. **`transcript = asr_model.transcribe([audio_path])[0]`**  \n",
        "   * `transcribe()` is a high-level helper that accepts a **list** of paths so it can batch multiple clips.  \n",
        "   * Under the hood it:\n",
        "     1. Loads each file and rescales it to mono 16 kHz.  \n",
        "     2. Converts waveforms to log-mel features.  \n",
        "     3. Runs the Conformer encoder → linear layer → CTC decoder.  \n",
        "     4. Performs greedy / beam search decoding to get text.  \n",
        "   * It returns a list of **Hypothesis** objects (one per clip) that hold `.text`, token IDs, and log-prob scores. The `[0]` grabs the first (and only) hypothesis.  ([NeMo/examples/asr/transcribe_speech.py at main - GitHub](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/transcribe_speech.py?utm_source=chatgpt.com), [NeMo/nemo/collections/asr/models/ctc_models.py at main - GitHub](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/asr/models/ctc_models.py?utm_source=chatgpt.com))\n",
        "\n",
        "7. **`print(\"Transcript:\", transcript.text)`**  \n",
        "   Prints the clean, lower-case English transcription—something like:\n",
        "\n",
        "   ```\n",
        "   Transcript: a king ruled the state in the early days\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "* **Latency & memory:** This small Conformer needs ~400 MB of GPU RAM and transcribes a 3-second clip in ≈40 ms on a T4.  \n",
        "* **Why CTC?** The model predicts characters all at once, so there’s no autoregressive loop—good for streaming or real-time apps.  \n",
        "* **Want timestamps?** `asr_model.transcribe([...], timestamps=True)` adds word- and char-level time offsets.  \n"
      ],
      "metadata": {
        "id": "ePtPXN65Kgw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q Cython packaging 'nemo_toolkit[asr]'\n",
        "!pip install numpy==1.24.3 -q --no-deps\n",
        "\n",
        "import nemo.collections.asr as nemo_asr\n",
        "print(\"✔ NeMo loaded inside venv:\", nemo_asr.__version__)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dyNVnkxuKg81",
        "outputId": "2413e22f-1d0d-41cb-b041-cc2589d8f3c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ NeMo loaded inside venv: 2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "\n",
        "# 13 M-parameter English model (~100 MB download, fits on a T4)\n",
        "asr_model = nemo_asr.models.ASRModel.from_pretrained(\n",
        "    \"nvidia/stt_en_conformer_ctc_small\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "p3_INRGKK7jy",
        "outputId": "8da6405b-ce38-4855-d700-f5f5a0da2058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-25 18:40:58 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-04-25 18:40:58 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: true\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 20.0\n",
            "    min_duration: 0.1\n",
            "    shuffle_n: 2048\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
            "    \n",
            "[NeMo W 2025-04-25 18:40:58 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n",
            "[NeMo W 2025-04-25 18:40:58 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-04-25 18:40:58 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-04-25 18:40:59 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /root/.cache/huggingface/hub/models--nvidia--stt_en_conformer_ctc_small/snapshots/e5b9941cc1b0b8a08c29b31a111c674f3040a80f/stt_en_conformer_ctc_small.nemo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: download a public 16 kHz sample (Harvard sentence, ~240 kB)\n",
        "!wget -q https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n",
        "\n",
        "# Option B (uncomment): upload your own .wav\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# audio_path = next(iter(uploaded))   # first uploaded file name\n",
        "audio_path = \"2086-149220-0033.wav\"   # comment out if you used Option B\n",
        "\n",
        "# Run inference (returns a list of hypotheses objects)\n",
        "transcript = asr_model.transcribe([audio_path])[0]\n",
        "print(\"Transcript:\", transcript.text)\n"
      ],
      "metadata": {
        "id": "_jeFq9ZCLIPf",
        "outputId": "bb3b6a5a-c8fd-4ae1-deba-b07e31ff714e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:00<00:00, 23.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript: well i don't wish to see it any more observed phoebe turning away her eyes it is certainly very like the old portrait\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}